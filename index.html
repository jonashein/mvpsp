<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Next-generation Surgical Navigation: Marker-less Multi-view 6DoF Pose Estimation of Surgical Instruments.">
    <meta name="keywords"
          content="MVPSP, Multi-view, RGB-D, Video, Dataset, 6DoF Pose Estimation, Surgical Instruments, Surgical Navigation, Surgery, Deep Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Next-generation Surgical Navigation: Marker-less Multi-view 6DoF Pose Estimation of Surgical Instruments</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="https://img.icons8.com/ios/100/surgical-scissors.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Next-generation Surgical Navigation: Marker-less Multi-view
                        6DoF Pose Estimation of Surgical Instruments</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Kk_o9AYAAAAJ" target="_blank" rel="noopener noreferrer">Jonas Hein</a><sup>1,2</sup>,</span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=ulEV9OkAAAAJ" target="_blank" rel="noopener noreferrer">Nicola Cavalcanti</a><sup>1</sup>,</span>
                        <span class="author-block">
              <a href="https://orcid.org/0000-0003-1916-5115" target="_blank" rel="noopener noreferrer">Daniel Suter</a><sup>3</sup>,
            </span>
                        <span class="author-block">
              <a href="https://orcid.org/0009-0002-7205-5257" target="_blank" rel="noopener noreferrer">Lukas Zingg</a><sup>3</sup>,
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=n7A302IAAAAJ" target="_blank" rel="noopener noreferrer">Fabio Carrillo</a><sup>1,4</sup>,
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=6JewdrMAAAAJ" target="_blank" rel="noopener noreferrer">Lilian Calvet</a><sup>4</sup>,
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=8vBn3bIAAAAJ" target="_blank" rel="noopener noreferrer">Mazda Farshad</a><sup>3</sup>
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=kzoVUPYAAAAJ" target="_blank" rel="noopener noreferrer">Nassir Navab</a><sup>5</sup>
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YYH0BjEAAAAJ" target="_blank" rel="noopener noreferrer">Marc Pollefeys</a><sup>2</sup>
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=nQ4B3BgAAAAJ" target="_blank" rel="noopener noreferrer">Philipp FÃ¼rnstahl</a><sup>1,4</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Research in Orthopedic Computer Science, Balgrist University Hospital, University of Zurich, Switzerland,</span>
                        <span class="author-block"><sup>2</sup>Computer Vision and Geometry Group, ETH Zurich, Switzerland,</span>
                        <span class="author-block"><sup>3</sup>Balgrist University Hospital, University of Zurich, Switzerland,</span>
                        <span class="author-block"><sup>4</sup>OR-X Translational Center for Surgery, Balgrist University Hospital, University of Zurich, Switzerland,</span>
                        <span class="author-block"><sup>5</sup>Computer Aided Medical Procedures, Technical University Munich, Germany</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                <a href="https://arxiv.org/abs/2305.03535"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <span class="link-block">
                <a href="https://arxiv.org/pdf/2305.03535"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                            <!-- Video Link.
                            <span class="link-block">
                              <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                                 class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="fab fa-youtube"></i>
                                </span>
                                <span>Video</span>
                              </a>
                            </span>
                            -->
                <span class="link-block">
                  <a href="https://github.com/jonashein/mvpsp_dataset"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset</span>
                    </a>
                </span>
                <!-- Dataset Link.
                            <span class="link-block">
                              <a href="https://github.com/google/nerfies/releases/tag/0.1"
                                 class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="far fa-images"></i>
                                </span>
                                <span>Data</span>
                                </a>
                           </span>
                           -->
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop is-centered">
        <div class="hero-body is-centered">
            <img src="./static/images/graphical_abstact.jpg"
                 alt="Graphical Abstract"/>
            <h2 class="subtitle has-text-centered">
                <p>A multi-view RGB-D video dataset of ex-vivo spine surgeries.</p>
                <p>Millimeter-accurate marker-less 6DoF pose estimation of surgical instruments.</p>
            </h2>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        State-of-the-art research of traditional computer vision is increasingly leveraged in the surgical domain.
                        A particular focus in computer-assisted surgery is to replace marker-based tracking systems for instrument localization with pure image-based 6DoF pose estimation using deep-learning methods.
                        However, state-of-the-art single-view pose estimation methods do not yet meet the accuracy required for surgical navigation.
                        In this context, we investigate the benefits of multi-view setups for highly accurate and occlusion-robust 6DoF pose estimation of surgical instruments and derive recommendations for an ideal camera system that addresses the challenges in the operating room.
                    </p>
                    <p>
                        Our contributions are threefold.

                        First, we present a multi-view RGB-D video dataset of ex-vivo spine surgeries, captured with static and head-mounted cameras and including rich annotations for surgeon, instruments, and patient anatomy.
                        Second, we perform an extensive evaluation of three state-of-the-art single-view and multi-view pose estimation methods, analyzing the impact of camera quantities and positioning, limited real-world data, and static, hybrid, or fully mobile camera setups on the pose accuracy, occlusion robustness, and generalizability.
                        Third, we design a multi-camera system for marker-less surgical instrument tracking, achieving an average position error of 1.01 mm and orientation error of 0.89Â° for a surgical drill, and 2.79 mm and 3.33Â° for a screwdriver under optimal conditions.
                        Our results demonstrate that marker-less tracking of surgical instruments is becoming a feasible alternative to existing marker-based systems.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->

        <!-- Dataset -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Dataset</h2>
                <p>We provide download and visualization scripts, and a Python wrapper for our dataset on Github: <a href="https://github.com/jonashein/mvpsp_dataset">https://github.com/jonashein/mvpsp_dataset</a></p>
            </div>
        </div>
        <!--/ Dataset -->

        <!-- Video -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Video</h2>
        <table>
                <tr>
                    <th></th>
                    <th style="text-align: center; vertical-align: middle;">OR-X Bright Test Set</th>
                    <th style="text-align: center; vertical-align: middle;">OR-X Dark Test Set</th></tr>
                <tr>
                    <th style="text-align: center; vertical-align: middle;">Synthetic Training</th>
                    <th style="text-align: center; vertical-align: middle;">
                        <video id="synthonly_orx_00003" autoplay muted loop playsinline height="97%" width="97%">
                            <source src="./static/videos/synthonly_orx_00003_overlay-converted-converted.mp4"
                                    type="video/mp4">
                        </video>
                    </th>
                    <th style="text-align: center; vertical-align: middle;">
                        <video id="synthonly_orx_10003" autoplay muted loop playsinline>
                            <source src="./static/videos/synthonly_orx_10003_overlay-converted-converted.mp4"
                                    type="video/mp4">
                        </video>
                    </th>
                <tr>
                    <th style="text-align: center; vertical-align: middle;">Synth-Real Training</th>
                    <th style="text-align: center; vertical-align: middle;">
                        <video id="synthreal_orx_00003" autoplay muted loop playsinline height="97%" width="97%">
                            <source src="./static/videos/synthreal_orx_00003_overlay-converted-converted.mp4"
                                    type="video/mp4">
                        </video>
                    </th>
                    <th style="text-align: center; vertical-align: middle;">
                        <video id="synthreal_orx_10003" autoplay muted loop playsinline>
                            <source src="./static/videos/synthreal_orx_10003_overlay-converted-converted.mp4"
                                    type="video/mp4">
                        </video>
                    </th>
                </tr>
            </table>
        </div></div>
        <!--/ Video -->

        <!-- Qualitative Results -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Qualitative Baselines Comparison</h2>
                <i>*yellow triangles indicate frames that were not part of the input to the pose estimation method.</i>
                <p>OR-X Bright Test Set</p>
                <img src="./static/images/bright_pose_estimates.jpg"
                 alt="Qualitative comparison on the bright OR-X subset."/>
                <p>OR-X Dark Test Set</p>
                <img src="./static/images/dark_pose_estimates.jpg"
                 alt="Qualitative comparison on the dark OR-X subset."/>
        </div></div>
        <!--/ Video -->
    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@misc{hein2023nextgeneration,
      title={Next-generation Surgical Navigation: Marker-less Multi-view 6DoF Pose Estimation of Surgical Instruments},
      author={Jonas Hein and Nicola Cavalcanti and Daniel Suter and Lukas Zingg and Fabio Carrillo and Lilian Calvet and Mazda Farshad and Marc Pollefeys and Nassir Navab and Philipp FÃ¼rnstahl},
      year={2023},
      eprint={2305.03535},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <a class="icon-link"
               href="https://arxiv.org/abs/2305.03535">
                <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="https://github.com/jonashein/mvpsp_dataset" class="external-link" disabled>
                <i class="fab fa-github"></i>
            </a>
        </div>
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
                        Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        Thanks to the authors of <a href="https://nerfies.github.io/">Nerfies</a> for providing this
                        nice website template!
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
