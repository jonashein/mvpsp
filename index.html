<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Next-generation Surgical Navigation: Marker-less Multi-view 6DoF Pose Estimation of Surgical Instruments.">
    <meta name="keywords"
          content="MVPSP, Multi-view, RGB-D, Video, Dataset, 6DoF Pose Estimation, Surgical Instruments, Surgical Navigation, Surgery, Deep Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Next-generation Surgical Navigation: Marker-less Multi-view 6DoF Pose Estimation of Surgical
        Instruments</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="https://img.icons8.com/ios/100/surgical-scissors.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Next-generation Surgical Navigation: Marker-less Multi-view
                        6DoF Pose Estimation of Surgical Instruments</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Kk_o9AYAAAAJ">Jonas Hein</a><sup>1,2</sup>,</span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=ulEV9OkAAAAJ">Nicola Cavalcanti</a><sup>1</sup>,</span>
                        <span class="author-block">
              <a href="https://orcid.org/0000-0003-1916-5115">Daniel Suter</a><sup>3</sup>,
            </span>
                        <span class="author-block">
              <a href="https://orcid.org/0009-0002-7205-5257">Lukas Zingg</a><sup>3</sup>,
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=n7A302IAAAAJ">Fabio Carrillo</a><sup>1,4</sup>,
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=6JewdrMAAAAJ">Lilian Calvet</a><sup>4</sup>,
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=8vBn3bIAAAAJ">Mazda Farshad</a><sup>3</sup>
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=kzoVUPYAAAAJ">Nassir Navab</a><sup>5</sup>
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YYH0BjEAAAAJ">Marc Pollefeys</a><sup>2</sup>
            </span>
                        <span class="author-block">
              <a href="https://scholar.google.com/citations?user=nQ4B3BgAAAAJ">Philipp F체rnstahl</a><sup>1,4</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Research in Orthopedic Computer Science, Balgrist University Hospital, University of Zurich, Switzerland,</span>
                        <span class="author-block"><sup>2</sup>Computer Vision and Geometry Group, ETH Zurich, Switzerland,</span>
                        <span class="author-block"><sup>3</sup>Balgrist University Hospital, University of Zurich, Switzerland,</span>
                        <span class="author-block"><sup>4</sup>OR-X Translational Center for Surgery, Balgrist University Hospital, University of Zurich, Switzerland,</span>
                        <span class="author-block"><sup>5</sup>Computer Aided Medical Procedures, Technical University Munich, Germany</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                <a href="https://arxiv.org/pdf/2305.03535"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                            <span class="link-block">
                <a href="https://arxiv.org/abs/2305.03535"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Video Link.
                            <span class="link-block">
                              <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                                 class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="fab fa-youtube"></i>
                                </span>
                                <span>Video</span>
                              </a>
                            </span>
                            -->
                            <!-- Code Link.
                            <span class="link-block">
                              <a href="https://github.com/google/nerfies"
                                 class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="fab fa-github"></i>
                                </span>
                                <span>Code</span>
                                </a>
                            </span>
                            -->
                            <!-- Dataset Link.
                            <span class="link-block">
                              <a href="https://github.com/google/nerfies/releases/tag/0.1"
                                 class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="far fa-images"></i>
                                </span>
                                <span>Data</span>
                                </a>
                           </span>
                           -->
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop is-centered">
        <div class="hero-body is-centered">
            <img src="./static/images/graphical_abstact.jpg"
                 alt="Graphical Abstract"/>
            <h2 class="subtitle has-text-centered">
                <p>A multi-view RGB-D video dataset of ex-vivo spine surgeries.</p>
                <p>Millimeter-accurate marker-less 6DoF pose estimation of surgical instruments.</p>
            </h2>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        State-of-the-art research of traditional computer vision is increasingly leveraged in the
                        surgical domain.
                        A particular focus in computer-assisted surgery is to replace marker-based tracking systems for
                        instrument
                        localization with pure image-based 6DoF pose estimation using deep-learning methods.
                        However, state-of-the-art single-view pose estimation methods do not yet meet the accuracy
                        required for surgical navigation.
                        In this context, we investigate the benefits of multi-view setups for highly accurate and
                        occlusion-robust
                        6DoF pose estimation of surgical instruments and derive recommendations for an ideal camera
                        system that
                        addresses the challenges in the operating room.
                    </p>
                    <p>
                        The contributions of this work are threefold.

                        First, we present a multi-camera capture setup consisting of static and head-mounted cameras,
                        which allows us to study the performance of pose estimation methods under various camera
                        configurations.
                        Second, we publish a multi-view RGB-D video dataset of ex-vivo spine surgeries,
                        captured in a surgical wet lab and a real operating theatre and including rich annotations for
                        surgeon,
                        instrument, and patient anatomy.
                        Third, we evaluate three state-of-the-art single-view and multi-view methods for the task of
                        6DoF pose estimation of surgical instruments and analyze the influence of camera configurations,
                        training data, and occlusions on the pose accuracy and generalization ability.
                        The best method utilizes five cameras in a multi-view pose optimization and achieves an average
                        position
                        and orientation error of 1.01 mm and 0.89째 for a surgical drill as well as 2.79 mm
                        and 3.33째 for a screwdriver under optimal conditions.
                        Our results demonstrate that marker-less tracking of surgical instruments is becoming a
                        feasible alternative to existing marker-based systems.
                    </p>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->

        <!-- Dataset -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Dataset</h2>
                <p>The dataset will be available here soon.</p>
            </div>
        </div>
        <!--/ Dataset -->

        <!-- Video -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Video</h2>
        <table>
                <tr>
                    <th></th>
                    <th style="text-align: center; vertical-align: middle;">OR-X Bright Test Set</th>
                    <th style="text-align: center; vertical-align: middle;">OR-X Dark Test Set</th></tr>
                <tr>
                    <th style="text-align: center; vertical-align: middle;">Synthetic Training</th>
                    <th style="text-align: center; vertical-align: middle;">
                        <video id="synthonly_orx_00003" autoplay muted loop playsinline height="97%" width="97%">
                            <source src="./static/videos/synthonly_orx_00003_overlay-converted-converted.mp4"
                                    type="video/mp4">
                        </video>
                    </th>
                    <th style="text-align: center; vertical-align: middle;">
                        <video id="synthonly_orx_10003" autoplay muted loop playsinline>
                            <source src="./static/videos/synthonly_orx_10003_overlay-converted-converted.mp4"
                                    type="video/mp4">
                        </video>
                    </th>
                <tr>
                    <th style="text-align: center; vertical-align: middle;">Synth-Real Training</th>
                    <th style="text-align: center; vertical-align: middle;">
                        <video id="synthreal_orx_00003" autoplay muted loop playsinline height="97%" width="97%">
                            <source src="./static/videos/synthreal_orx_00003_overlay-converted-converted.mp4"
                                    type="video/mp4">
                        </video>
                    </th>
                    <th style="text-align: center; vertical-align: middle;">
                        <video id="synthreal_orx_10003" autoplay muted loop playsinline>
                            <source src="./static/videos/synthreal_orx_10003_overlay-converted-converted.mp4"
                                    type="video/mp4">
                        </video>
                    </th>
                </tr>
            </table>
        </div></div>
        <!--/ Video -->

        <!-- Qualitative Results -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Qualitative Baselines Comparison</h2>
                <i>*yellow triangles indicate frames that were not part of the input to the pose estimation method.</i>
                <p>OR-X Bright Test Set</p>
                <img src="./static/images/bright_pose_estimates.jpg"
                 alt="Qualitative comparison on the bright OR-X subset."/>
                <p>OR-X Dark Test Set</p>
                <img src="./static/images/dark_pose_estimates.jpg"
                 alt="Qualitative comparison on the dark OR-X subset."/>
        </div></div>
        <!--/ Video -->
    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@misc{hein2023nextgeneration,
      title={Next-generation Surgical Navigation: Marker-less Multi-view 6DoF Pose Estimation of Surgical Instruments},
      author={Jonas Hein and Nicola Cavalcanti and Daniel Suter and Lukas Zingg and Fabio Carrillo and Lilian Calvet and Mazda Farshad and Marc Pollefeys and Nassir Navab and Philipp F체rnstahl},
      year={2023},
      eprint={2305.03535},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <a class="icon-link"
               href="https://arxiv.org/abs/2305.03535">
                <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="https://github.com/jonashein" class="external-link" disabled>
                <i class="fab fa-github"></i>
            </a>
        </div>
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        Thanks to the authors of <a href="https://nerfies.github.io/">Nerfies</a> for providing this
                        nice website template!
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
